#Start Spark

from pyspark.sql import SparkSession

# Start spark and settings  'spark.sql.debug.maxToStringFields'
# to prevent trunction if it is more than 1000
spark = SparkSession.builder.appName("ClassificationEda") \
        .config("spark.sql.debug.maxToStringFields", 1000) \
        .getOrCreate()

#to use col function
from pyspark.sql.functions import col, when
# to draw corellation matrix
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation
import matplotlib.pyplot as plt
# Check Version
spark


# Load Datasets
target = spark.read.csv("/user/hduser/data/target.csv", header=True, inferSchema=True)
target.show(5)

#Check Schema

target.printSchema()

# Missing Value

columns_to_check = ["id", "flag"]

# calculate missing value 
for column in columns_to_check:
    missing_values_count = target.filter(col(column).isNull()).count()
    print(f"Column '{column}': {missing_values_count} missing value")

# Load DAtaset

df0 = spark.read.parquet("/user/hduser/data/train_data_0.pq", header=True, inferSchema=True)
#df1 = spark.read.parquet("/user/hduser/data/train_data_1.pq", header=True, inferSchema=True)
#df2 = spark.read.parquet("/user/hduser/data/train_data_2.pq", header=True, inferSchema=True)
#df3 = spark.read.parquet("/user/hduser/data/train_data_3.pq", header=True, inferSchema=True)
#df4 = spark.read.parquet("/user/hduser/data/train_data_4.pq", header=True, inferSchema=True)
#df5 = spark.read.parquet("/user/hduser/data/train_data_5.pq", header=True, inferSchema=True)
#df6 = spark.read.parquet("/user/hduser/data/train_data_6.pq", header=True, inferSchema=True)
#df7 = spark.read.parquet("/user/hduser/data/train_data_7.pq", header=True, inferSchema=True)
#df8 = spark.read.parquet("/user/hduser/data/train_data_8.pq", header=True, inferSchema=True)
#df9 = spark.read.parquet("/user/hduser/data/train_data_9.pq", header=True, inferSchema=True)
#df10 =spark.read.parquet("/user/hduser/data/train_data_10.pq", header=True, inferSchema=True)
#df11= spark.read.parquet("/user/hduser/data/train_data_11.pq", header=True, inferSchema=True)

#check data

# Save dataframes to pyspark catalog
df0.createOrReplaceTempView("df0")
#df1.createOrReplaceTempView("df1")
#df2.createOrReplaceTempView("df2")
#df3.createOrReplaceTempView("df3")
#df4.createOrReplaceTempView("df4")
#df5.createOrReplaceTempView("df5")
#df6.createOrReplaceTempView("df6")
#df7.createOrReplaceTempView("df7")
#df8.createOrReplaceTempView("df8")
#df9.createOrReplaceTempView("df9")
#df10.createOrReplaceTempView("df10")
#df11.createOrReplaceTempView("df11")
df0.head(1)

# Combined all dataframes
#combined_df = None
#for i in range(12)
#for i in range(12):
 #   df_name = f"df{i}"
  #  if combined_df is None:
   #     combined_df = spark.table(df_name)
   # else:
   #     combined_df = combined_df.union(spark.table(df_name))

# show
#combined_df.show(1)

#filter data
combined_df = df0.limit(500000)

data_type = type(combined_df)

#check schema
combined_df.printSchema()


from pyspark.sql.functions import col

# convert all columsn to integer
for col_name in combined_df.columns:
    combined_df = combined_df.withColumn(col_name, col(col_name).cast("integer"))

# recent Schema
combined_df.printSchema()

#check columns name

combined_df.columns

# select columns
#"pre_loans_total_overdue",
selected_columns = ["id","pre_loans_credit_limit", "pre_loans_next_pay_summ", "pre_loans_outstanding","pre_loans_credit_cost_rate"
                   ,"pre_util","pre_over2limit","pre_maxover2limit","enc_loans_credit_type"]
selected_df0 = combined_df.select(*selected_columns)

# Show results
selected_df0.show(1)

#Check missing value

from pyspark.sql.functions import isnan, when, count, col
selected_df0.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in selected_df0.columns]).show()

#no missing value

# Summary of the dataset

#,"pre_loans_total_overdue"
selected_df0.describe(["pre_loans_credit_limit", "pre_loans_next_pay_summ", "pre_loans_outstanding"]).show()

selected_df0.describe(["pre_loans_credit_cost_rate"
                   ,"pre_util","pre_over2limit","pre_maxover2limit","enc_loans_credit_type"]).show()

rows = selected_df0.count()
print(f"DataFrame Rows count : {rows}")

#merge all data 
full_data = selected_df0.join(target, "id", "inner")

# Birleştirilmiş veriyi gösterme
full_data.show(1)

full_data.printSchema()

#data type check
data_type = type(full_data)
print(data_type)

#data counts and summary
full_data.describe(["id","pre_loans_credit_limit","pre_loans_next_pay_summ","flag"]).show()

from pyspark.sql.functions import col, count
full_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in full_data.columns]).show()

# Her sütundaki eksik değer sayısını hesaplayın ve sütun adını koruyun
missing_values = full_data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in full_data.columns])

# Eksik değer sayılarını görüntüleyin
missing_values.show()

#Memory space error

#data counts and summary
full_data.describe(["id","pre_loans_credit_limit","pre_loans_next_pay_summ","flag"]).show()

#Duplicate
duplicate_rows = full_data.groupBy(full_data.columns).count().filter(col("count") > 1)

# Show results
duplicate_rows.show()

# drop dublicate
full_data = full_data.dropDuplicates()

# show dataframe
full_data.show(1)

#check result
duplicate_rows2 = full_data.groupBy(full_data.columns).count().filter(col("count") > 1)

# show result
duplicate_rows2.show()

#corellation
# take columns
feature_columns = full_data.columns
vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
assembled_df = vector_assembler.transform(full_data)

# calculate matrix
correlation_matrix = Correlation.corr(assembled_df, "features").collect()[0][0]

# visulasation
plt.figure(figsize=(8, 8))
plt.imshow(correlation_matrix.toArray(), cmap='coolwarm', interpolation='nearest')
plt.colorbar()

# put on the values on the map 
num_cols = len(feature_columns)
for i in range(num_cols):
    for j in range(num_cols):
        plt.text(i, j, f'{correlation_matrix[i, j]:.2f}', ha='center', va='center', color='black')

plt.title("Corellation matrix")
plt.xticks(range(num_cols), feature_columns, rotation=45)
plt.yticks(range(num_cols), feature_columns)
plt.show()

from pyspark.sql.functions import avg
# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_loans_credit_limit").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_loans_next_pay_summ").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_loans_outstanding").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
#result = full_data.groupBy("flag").agg(avg("pre_loans_total_overdue").alias("Average_Value"))

# Show result
#result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_loans_credit_cost_rate").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_util").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_over2limit").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("pre_maxover2limit").alias("Average_Value"))

# Show result
result.show()

# group by and avg value
result = full_data.groupBy("flag").agg(avg("enc_loans_credit_type").alias("Average_Value"))

# Show result
result.show()

full_data.show(5,truncate=False)

full_data.printSchema()

#Normalization (Min Max)
#I choosed Min-Max Because my data is in the range of 0-1 and I do not want to keep the distribution of the data

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, MinMaxScaler
from pyspark.ml.classification import LogisticRegression


# drop first and last column
feature_cols = full_data.columns[1:-1]  # İlk sütun ve son sütun dışında kalan özellik sütunlarını seçin
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features5")
full_data = assembler.transform(full_data)

# apply min max normalization
scaler = MinMaxScaler(inputCol="features5", outputCol="scaled_features")
scaler_model = scaler.fit(full_data)
data = scaler_model.transform(full_data)

train_data, test_data = data.randomSplit([0.7, 0.3], seed=123)

train_data.show(1)

test_data.show(1)

# Logistic regression
lr = LogisticRegression(featuresCol="scaled_features", labelCol="flag")
model = lr.fit(train_data)


# Modeli değerlendirme
from pyspark.ml.evaluation import BinaryClassificationEvaluator

predictions = model.transform(test_data)
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="flag")
accuracy = evaluator.evaluate(predictions)

print("Model Accuracy:", accuracy)

#0.5667 accuracy

from pyspark.ml.classification import MultilayerPerceptronClassifier
# Yapay sinir ağı modeli için katmanları tanımlayın
# Örnek olarak [giriş katmanı, gizli katmanlar, çıkış katmanı] şeklinde bir liste verebilirsiniz
layers = [len(train_data.select("scaled_features").first()[0]), 10, 5, 2]  # Örnekte gizli katmanlar 2 tane
# varsayilan ogrenme yontemi 0.03 (stepSize)
#varsayilan solver "l-bfgs",  # Geri yayılım algoritması olarak L-BFGS(Limited-memory Broyden-Fletcher-Goldfarb-Shanno)
# MultilayerPerceptronClassifier modelini tanımlayın
mlp = MultilayerPerceptronClassifier(
    layers=layers, 
    labelCol="flag", 
    featuresCol="scaled_features",
    maxIter=100, 
    blockSize=128, 
    seed=1234)

model = mlp.fit(train_data)

predictions = model.transform(test_data)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol="flag", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)




